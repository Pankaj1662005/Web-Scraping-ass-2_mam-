# -*- coding: utf-8 -*-
"""assignment2_mam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13iN5uk2YYKP-M-RGRF_lCbkC6WQDtpES
"""

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/Drake.csv")

# Display a preview
df.head(2)

"""# Clean the Data"""

df = df.drop(columns=['Unnamed: 0', 'Artist', 'Date']) #Drop Unnecessary Columns

df.head(2)

# Rename column for clarity
df = df.rename(columns={'Lyric': 'lyrics'})

df['lyrics'] = df['lyrics'].str.lower()

df.lyrics

import string
df['lyrics'] = df['lyrics'].str.translate(str.maketrans('', '', string.punctuation))
 #Remove punctuation:

df['lyrics'] = df['lyrics'].str.replace(r'\d+', '', regex=True)
  #Remove numerical values:

df['lyrics'] = df['lyrics'].str.replace(r'\n', ' ', regex=True)
 #Remove non-sensical text like \n:

!pip install spacy

!python -m spacy download en_core_web_sm

import spacy

# Load the spaCy model
nlp = spacy.load('en_core_web_sm')

# Function to remove stopwords using spaCy tokenizer
def remove_stopwords_spacy(text):
    if isinstance(text, str):
        doc = nlp(text)  # Tokenize the text using spaCy
        return [token.text for token in doc if not token.is_stop]  # Remove stopwords
    return []

# Apply the function to the cleaned lyrics
df['tokens_without_stopwords'] = df['cleaned_lyrics'].apply(remove_stopwords_spacy)

df[['lyrics', 'cleaned_lyrics', 'tokens_without_stopwords']].head()

# Create the corpus: collection of all cleaned lyrics as text
corpus = df['cleaned_lyrics'].tolist()

# Print the first few entries in the corpus
print(corpus[:5])

# Check for missing values in the 'cleaned_lyrics' column
print(df['cleaned_lyrics'].isnull().sum())

# Remove rows where 'cleaned_lyrics' is NaN
df_cleaned = df.dropna(subset=['cleaned_lyrics'])

# Now, create the corpus again
corpus = df_cleaned['cleaned_lyrics'].tolist()

from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the cleaned corpus to get the Document-Term Matrix (DTM)
dtm = vectorizer.fit_transform(corpus)

# Convert the DTM to a pandas DataFrame for better readability
dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())

# Display the DTM DataFrame
print(dtm_df.head())

import re

def clean_text_round2(text):
    # Lowercase the text
    text = text.lower()

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove common non-sensical text like newline characters
    text = re.sub(r'\n|\r', ' ', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # Remove email addresses
    text = re.sub(r'\S+@\S+', '', text)

    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)

    return text

# Step 1: Replace NaN values in the 'lyrics' column with empty strings
df['lyrics'].fillna('', inplace=True)

# Step 2: Apply the cleaning function to the 'lyrics' column
df['cleaned_lyrics'] = df['lyrics'].apply(clean_text_round2)

# Verify the changes
print(df[['lyrics', 'cleaned_lyrics']].head())



"""# Scrapping basic without url

Step 1: Import Required Libraries
"""

import pandas as pd  # For working with DataFrames
import string  # For handling punctuation
import re  # For regular expressions (e.g., removing numbers)
from nltk.tokenize import word_tokenize  # For splitting text into words (tokenization)
from nltk.corpus import stopwords  # For removing common stop words (e.g., "the", "is")
import nltk  # Natural Language Toolkit for text processing

# Download NLTK resources
nltk.download('punkt')  # For tokenization
nltk.download('stopwords')  # For stop words

"""Step 2: Load the Transcripts"""

# Example DataFrame with sample transcripts
data = {
    'transcript': [
        "Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.",
        "Machine learning (ML) is a type of artificial intelligence (AI) that allows computers to learn from data and improve their performance over time. ML uses algorithms to identify patterns in data, which are then used to make predictions.it was founded in 1960 "
    ]
}
df = pd.DataFrame(data)  # Create a DataFrame from the sample data

print(data)

"""Step 3: Data Cleaning and Preprocessing"""

df['cleaned_transcript'] = df['transcript'].str.lower()  # Convert text to lowercase

# Create a translation table to remove punctuation
df['cleaned_transcript'] = df['cleaned_transcript'].str.translate(str.maketrans('', '', string.punctuation))

# Use regex to replace all digits with an empty string
df['cleaned_transcript'] = df['cleaned_transcript'].apply(lambda x: re.sub(r'\d+', '', x))   #apply ka amtlb har ek row per apply karo  #\d repersent any diigit btwn 0 to 9  # lambda x is function defined by us for this
#Har row ka text regex ke through clean ho raha hai.

# Replace newline characters with a space
df['cleaned_transcript'] = df['cleaned_transcript'].str.replace('\n', ' ')

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy
nlp_mera = spacy.load("en_core_web_sm")
# Use Spacy to tokenize the text
df['tokens'] = df['cleaned_transcript'].apply(lambda text: [token.text for token in nlp_mera(text)])

print(df)

# Load the set of English stop words
stop_words = set(stopwords.words('english'))

# Remove stop words from the tokens
df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])

df.head(2)

"""Step 5: Analyze Similarities and Differences
Once the data is cleaned and tokenized, you can analyze patterns, similarities, and differences using techniques such as:
"""

from collections import Counter  # For counting word frequencies

# Flatten the list of tokens into a single list
all_tokens = [token for sublist in df['tokens'] for token in sublist]

# Count the frequency of each word
word_freq = Counter(all_tokens)

# Print the 10 most common words
print(word_freq.most_common(10))

from sklearn.feature_extraction.text import TfidfVectorizer  # For TF-IDF calculation

# Create a TF-IDF vectorizer
tfidf = TfidfVectorizer()

# Fit and transform the cleaned transcripts into a TF-IDF matrix
tfidf_matrix = tfidf.fit_transform(df['cleaned_transcript'])

print(tfidf_matrix)

print("easy view of above-> ")
# Convert the TF-IDF matrix to a DataFrame for easier viewing
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())
tfidf_df

from sklearn.metrics.pairwise import cosine_similarity  # For calculating cosine similarity

# Compute the cosine similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

# Print the similarity matrix
print(similarity_matrix)

from sklearn.decomposition import LatentDirichletAllocation  # For topic modeling

# Create an LDA model with 5 topics
lda = LatentDirichletAllocation(n_components=5, random_state=42)

# Fit the LDA model to the TF-IDF matrix
lda.fit(tfidf_matrix)

# Print the top words for each topic
for idx, topic in enumerate(lda.components_):
    print(f"Topic {idx}:")
    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-5:]])  # Top 5 words per topic

import matplotlib.pyplot as plt  # For plotting

plt.figure(figsize=(50, 10))
# Plot the word frequencies
plt.bar(word_freq.keys(), word_freq.values())
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Word Frequency Analysis')
plt.show()



"""# Scrape and analyze data analyst job requirements with Python **ðŸ˜ŠðŸ˜ŠðŸ˜ŠðŸ˜Š**(best for learning beautiful soup)

1) When you run this code, it issues an HTTP GET request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object you called page.
"""

import requests

URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)

print(page.text)

"""2)Youâ€™ve successfully scraped some HTML from the internet, but when you look at it, it looks like a mess. There are tons of HTML elements here and there, thousands of attributes scattered aroundâ€”and maybe thereâ€™s some JavaScript mixed in as well? Itâ€™s time to parse this lengthy code response with the help of Python to make it more accessible so you can pick out the data that you want

"""

import requests
from bs4 import BeautifulSoup

URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)

soup = BeautifulSoup(page.content, "html.parser")

#TRY WITH ID NAME
results = soup.find(id="ResultsContainer")

print(results.prettify())

#FIND ALL BY DIV AND CLASS NAME
job_cards = results.find_all("div", class_="card-content")

for job_card in job_cards:
  print(job_card, end="\n" * 2)

#Find Elements by HTML Class Name
>>> for job_card in job_cards:
...     title_element = job_card.find("h2", class_="title")
...     company_element = job_card.find("h3", class_="company")
...     location_element = job_card.find("p", class_="location")
...     print(title_element)
...     print(company_element)
...     print(location_element)
...     print()

#Extract Text From HTML Elements
>>> for job_card in job_cards:
...     title_element = job_card.find("h2", class_="title")
...     company_element = job_card.find("h3", class_="company")
...     location_element = job_card.find("p", class_="location")
...     print(title_element.text)
...     print(company_element.text)
...     print(location_element.text)
...     print()

>>> for job_card in job_cards:
...     title_element = job_card.find("h2", class_="title")
...     company_element = job_card.find("h3", class_="company")
...     location_element = job_card.find("p", class_="location")
...     print(title_element.text.strip())
...     print(company_element.text.strip())
...     print(location_element.text.strip())
...     print()

import requests
from bs4 import BeautifulSoup

URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)

soup = BeautifulSoup(page.content, "lxml")
results = soup.find(id="ResultsContainer")

python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)

python_job_cards = [
    h2_element.parent.parent.parent for h2_element in python_jobs
]

for job_card in python_job_cards:
    title_element = job_card.find("h2", class_="title")
    company_element = job_card.find("h3", class_="company")
    location_element = job_card.find("p", class_="location")
    print(title_element.text.strip())
    print(company_element.text.strip())
    print(location_element.text.strip())
    link_url = job_card.find_all("a")[1]["href"]
    print(f"Apply here: {link_url}\n")
    print("\n")

"""# seema mam assin-3"""

import requests  # Web se data fetch karne ke liye
from bs4 import BeautifulSoup  #from bs4 import BeautifulSoup imports the BeautifulSoup class from the bs4 package.
                                #BeautifulSoup allows you to parse, navigate, and modify HTML/XML content effortlessly.


# Step 1: Website ka URL define karo
url = "http://quotes.toscrape.com"

# Step 2: Website ko request bhejo (GET request)
response = requests.get(url)

# Step 3: HTML content ko parse karo
soup = BeautifulSoup(response.text, 'lxml')
# or soup=beautifulsoup(htm_doc ,"html.parser")

# Step 4: Sabhi quotes dhundo (jo `<div class='quote'>` ke andar hain) &&&  div â†’ Yeh HTML tag hai jisme quotes likhe hote hain.
quotes = soup.find_all('div', class_='quote')

# Step 5: Data extract karo
quote_data = []
for quote in quotes:
    text = quote.find('span', class_='text').text  # Quote ka text nikalna
    author = quote.find('small', class_='author').text  # Author ka naam nikalna
    quote_data.append((author, text))  # List me store karna

# Step 6: Pehle 3 quotes print karna
print("Raw Extracted Data:")
for q in quote_data[:3]:  # Sirf pehle 3 quotes dikhana
    print(q)

print(response.text) # raw HTML

print(soup.find_all('div')) #  parsed HTML extraction, Clean & structured,Only <div> tags,   -----> <div> tag Groups elements together for styling and layout

print(response.status_code) #Prints the HTTP status code

import re

def clean_text(text ):

    #'replace this'," with this",""
    #re.sub() ek function hai jo Python ke re (Regular Expressions) module ka part hai.-> Yeh text me se unwanted words, symbols ya patterns ko replace karne ke liye use hota hai.
    # sub() -> it is substitue function
    #re.sub(pattern, replacement, string)


    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text)

    # Remove special characters (only keep alphanumeric characters and spaces)
    text = re.sub(r'[^a-zA-Z\s]', '', text)  #[^a-zA-Z\s] â†’ Matches anything except letters (a-z, A-Z) and spaces (\s).

    # Remove URLs
    text = re.sub(r'http[s]?://\S+', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove single-character words
    text = re.sub(r'\b\w\b', '', text)

    # Convert to lowercase and strip leading/trailing spaces
    text = text.lower().strip()
    return text

# Apply cleaning function to quotes
cleaned_quote_data = [(author, clean_text(text)) for author, text in quote_data]

# Display cleaned data
print("Cleaned Data:")
for q in cleaned_quote_data[:3]:  # Show first 3 quotes
    print(q)

import pandas as pd

# Create a pandas DataFrame
df = pd.DataFrame(cleaned_quote_data, columns=["Author", "Quote"])

# Display first few rows
print("Corpus (DataFrame):")
print(df.head())

#CountVectorizer ek tool hai jo text data ko numbers me convert karta hai taaki hum uspar machine learning ya data analysis kar sakein.
#Yeh word frequency count karta hai aur ek matrix me store karta hai, jise Document-Term Matrix (DTM) kehte hain.

from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer
#min_df=2 â†’ Sirf wahi words rakhega jo kam se kam 2 documents me aate hain.
#max_df=0.9 â†’ Agar koi word 90% ya usse zyada documents me hai, toh usko ignore karega (kyunki wo common word hoga).
vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.9)

# Fit and transform the text data
X = vectorizer.fit_transform(df['Quote'])

# Convert to DataFrame
dtm = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Display Document-Term Matrix
print("\nDocument-Term Matrix (DTM):")
dtm.head()

import pickle

# Save the Document-Term Matrix (DTM) as a pickle file
dtm.to_pickle("dtm.pkl")

# Save the cleaned corpus (DataFrame) as a pickle file
df.to_pickle("corpus.pkl")

print("DTM and Corpus have been saved as .pkl files!")

# Load the Document-Term Matrix
loaded_dtm = pd.read_pickle("dtm.pkl")

# Load the cleaned corpus (quotes and authors)
loaded_corpus = pd.read_pickle("corpus.pkl")

print("DTM and Corpus successfully loaded from .pkl files!")